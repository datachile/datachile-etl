{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>MAIN CODE</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/mauricio/datachile-etl/childhood/dropout_rate'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/Users/mauricio/datachile-etl/childhood/dropout_rate\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00 s | Opening http://www.creciendoconderechos.gob.cl/docs/Rendimiento_Escolar_Basica.xlsx\n",
      "1.77 s | Columns selected.\n",
      "1.77 s | Melted columns.\n",
      "1.87 s | Year column created.\n",
      "2.03 s | Status column created.\n",
      "2.03 s | Dropped extra columns and NaN values.\n",
      "2.15 s | Education column created.\n",
      "2.15 s | Appended dataframe.\n",
      "2.15 s | Opening http://www.creciendoconderechos.gob.cl/docs/Rendimiento_Escolar_Media.xlsx\n",
      "3.36 s | Columns selected.\n",
      "3.36 s | Melted columns.\n",
      "3.46 s | Year column created.\n",
      "3.61 s | Status column created.\n",
      "3.62 s | Dropped extra columns and NaN values.\n",
      "3.73 s | Education column created.\n",
      "3.73 s | Appended dataframe.\n",
      "3.73 s | Concatenated each dataframe.\n",
      "4.12 s | Exported CSV file.\n",
      "4.12 s | Exported status.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# declares timer, locks absolute_path to working directory, declares file URLs and list of df\n",
    "before = time.perf_counter()\n",
    "absolute_path = os.getcwd()\n",
    "urls = [\"http://www.creciendoconderechos.gob.cl/docs/Rendimiento_Escolar_Basica.xlsx\", \"http://www.creciendoconderechos.gob.cl/docs/Rendimiento_Escolar_Media.xlsx\"]\n",
    "df_list = []\n",
    "\n",
    "# creates data_temp folder and changes working directory\n",
    "if os.path.isdir(\"data_temp\") == False:\n",
    "    os.mkdir(\"data_temp\")    \n",
    "os.chdir(\"data_temp\")\n",
    "\n",
    "# generates header row\n",
    "hrow = [\"region_code\", \"region_name\", \"province_code\", \"province_name\", \"commune_code\", \"commune_name\"]\n",
    "for yr in range(2010,2018):\n",
    "    hrow.append(\"total_\"+str(yr))\n",
    "    hrow.append(\"prom_num_\"+str(yr))\n",
    "    hrow.append(\"prom_perc_\"+str(yr))\n",
    "    hrow.append(\"rep_num_\"+str(yr))\n",
    "    hrow.append(\"rep_perc_\"+str(yr))\n",
    "    hrow.append(\"drop_num_\"+str(yr))\n",
    "    hrow.append(\"drop_perc_\"+str(yr))\n",
    "\n",
    "# processing\n",
    "for url in urls:\n",
    "    print(\"{:.2f} s | Opening {}\".format(time.perf_counter()-before, url))\n",
    "    df = pd.read_excel(url, header = None, sheet_name = \"Información Base Comunal\", skiprows = list(range(5)))\n",
    "    df.columns = hrow\n",
    "\n",
    "    # selects necessary columns\n",
    "    sel_cols = [\"commune_code\"]\n",
    "    for yr in range(2010,2018):\n",
    "        sel_cols.append(\"prom_num_\"+str(yr))\n",
    "        sel_cols.append(\"rep_num_\"+str(yr))\n",
    "        sel_cols.append(\"drop_num_\"+str(yr))\n",
    "\n",
    "    df = df[sel_cols]\n",
    "    print(\"{:.2f} s | Columns selected.\".format(time.perf_counter()-before))\n",
    "\n",
    "    # melts columns to make dataframe tidy\n",
    "    melt_cols = [col for col in df.columns if col != \"commune_code\"]\n",
    "    df = pd.melt(df, id_vars = \"commune_code\", value_vars = melt_cols, var_name = \"status_year\", value_name = \"total\")\n",
    "    print(\"{:.2f} s | Melted columns.\".format(time.perf_counter()-before))\n",
    "    \n",
    "    # creates year column\n",
    "    def get_year(row, col):\n",
    "        target = row[col]\n",
    "        reg = re.search(\"\\d\", target)\n",
    "        first = reg.start()\n",
    "        y = target[first : first + 4]\n",
    "        return y\n",
    "\n",
    "    df[\"year\"] = df.apply(get_year, col = \"status_year\", axis = 1)\n",
    "    print(\"{:.2f} s | Year column created.\".format(time.perf_counter()-before))\n",
    "    \n",
    "    # creates status column\n",
    "    def get_status(row, col):\n",
    "        stat = {\"prom\": 1, \"rep\": 2, \"drop\": 3}\n",
    "        return next((stat[k] for k in stat.keys() if k in row[col]), np.nan)\n",
    "\n",
    "    df[\"status_id\"] = df.apply(get_status, col = \"status_year\", axis = 1)\n",
    "    print(\"{:.2f} s | Status column created.\".format(time.perf_counter()-before))\n",
    "\n",
    "    # drops status_year column and NaN rows on commune_id\n",
    "    df = df[[c for c in df.columns if c != \"status_year\"]]\n",
    "    df = df.dropna(subset = [\"commune_code\"])\n",
    "    print(\"{:.2f} s | Dropped extra columns and NaN values.\".format(time.perf_counter()-before))\n",
    "\n",
    "    # creates education column\n",
    "    ed = re.search(\"_(.+?)_(.+?).xlsx\", url)\n",
    "    ed = ed.group(2)\n",
    "    df[\"education_id\"] = pd.Series([ed] * len(df.index)).values\n",
    "    \n",
    "    def get_education(row, col):\n",
    "        ed_dict = {\"Basica\": 1, \"Media\": 2}\n",
    "        return ed_dict[row[col]]\n",
    "    \n",
    "    df[\"education_id\"] = df.apply(get_education, col = \"education_id\", axis = 1)\n",
    "    print(\"{:.2f} s | Education column created.\".format(time.perf_counter()-before))\n",
    "\n",
    "    df_list.append(df)\n",
    "    print(\"{:.2f} s | Appended dataframe.\".format(time.perf_counter()-before))\n",
    "    \n",
    "# concatenates each year's dataframe\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "print(\"{:.2f} s | Concatenated each dataframe.\".format(time.perf_counter()-before))\n",
    "\n",
    "# writes datachile official IDs for each commune and drops unnecesary columns\n",
    "df_ids = pd.read_csv(\"https://raw.githubusercontent.com/datachile/datachile-etl/master/official_ids/2017_06_27_comunas_datachile_fixed.csv\")\n",
    "df = pd.merge(df, df_ids, left_on = \"commune_code\", right_on = \"comuna_customs_id\")\n",
    "df = df[[\"comuna_datachile_id\", \"year\", \"education_id\", \"status_id\", \"total\"]]\n",
    "df = df.rename(columns = {\"comuna_datachile_id\": \"comuna_id\"})\n",
    "\n",
    "# fills NaN values in total column\n",
    "df[\"total\"] = df[\"total\"].replace(\"-\",0)\n",
    "\n",
    "# converts all columns to integer type\n",
    "df[df.columns] = df[df.columns].apply(pd.to_numeric, downcast = \"integer\")\n",
    "\n",
    "# comes back to original path, creates data_final folder and exports as csv\n",
    "os.chdir(absolute_path)\n",
    "if os.path.isdir(\"data_final\") == False:\n",
    "    os.mkdir(\"data_final\")    \n",
    "os.chdir(\"data_final\")\n",
    "df.to_csv(\"dropout_rate.csv\", index = False)\n",
    "print(\"{:.2f} s | Exported CSV file.\".format(time.perf_counter()-before))\n",
    "\n",
    "# creates CSV with priority IDs\n",
    "stat_tb = {\"id\": list(range(1,4)), \"name_es\": [\"Aprobados\", \"Reprobados\", \"Abandonos\"], \"name_en\": [\"Promoted\", \"Repeated\", \"Dropouts\"]}\n",
    "stat_df = pd.DataFrame(stat_tb)\n",
    "stat_df.to_csv(\"status.csv\", index = False)\n",
    "print(\"{:.2f} s | Exported status.csv\".format(time.perf_counter()-before))\n",
    "\n",
    "# comes back to original path\n",
    "os.chdir(absolute_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>IVE JUNAEB Code</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "from urllib.parse import urlparse\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import urllib3\n",
    "\n",
    "# declares timer and locks absolute path to working directory\n",
    "before = time.perf_counter()\n",
    "absolute_path = os.getcwd()\n",
    "\n",
    "# solves SSL certificate issues error when retrieving files \n",
    "#import os, ssl\n",
    "#if (not os.environ.get(\"PYTHONHTTPSVERIFY\", \"\") and getattr(ssl, \"_create_unverified_context\", None)): \n",
    "#    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Solving SSL certificate issue\n",
    "#urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# retrieve links from site\n",
    "url = \"https://www.junaeb.cl/ive\"\n",
    "page = requests.get(url, verify = False)\n",
    "soup = BeautifulSoup(page.text, \"lxml\")\n",
    "a_tags = soup.find_all('a')\n",
    "all_links = [link.get('href') for link in a_tags]\n",
    "links = []\n",
    "for link in all_links:\n",
    "    if \".xls\" in link and \"IVESINAE_COMUNA_2013\" not in link:\n",
    "        links.append(link)\n",
    "        \n",
    "years_df = []\n",
    "\n",
    "# creates data_temp folder and changes working directory\n",
    "if os.path.isdir(\"data_temp\") == False:\n",
    "    os.mkdir(\"data_temp\")    \n",
    "os.chdir(\"data_temp\")\n",
    "\n",
    "# downloads files\n",
    "for url in links:\n",
    "    encoded_url = urllib.parse.quote(url.encode('utf-8'),':/')\n",
    "    filename = encoded_url[encoded_url.rfind(\"/\")+1:]\n",
    "    if filename not in os.listdir():\n",
    "        urllib.request.urlretrieve(encoded_url, filename)\n",
    "        print(\"{:.2f} s | Downloaded {}\".format(time.perf_counter()-before, url))\n",
    "\n",
    "# processing function\n",
    "def clean(file, year):\n",
    "    \n",
    "    #defines scope of variable\n",
    "    global years_df\n",
    "        \n",
    "    # reads Excel file and defines available sheets on file\n",
    "    df = pd.read_excel(file, sheet_name = None, encoding=\"utf-8\")\n",
    "    sheets_list = list(df.keys())\n",
    "    sheets = sheets_list[:2]\n",
    "\n",
    "    for tb in sheets:\n",
    "        # deletes last two columns and last row\n",
    "        df[tb] = df[tb].drop(df[tb].columns[14:], axis=1)\n",
    "        last_row = df[tb].shape[0]\n",
    "        df[tb] = df[tb].drop(last_row-1)\n",
    "\n",
    "        # fills NaN with 0\n",
    "        df[tb].iloc[:, 9:] = df[tb].iloc[:, 9:].fillna(0)\n",
    "\n",
    "        # creates new column to make dataframe tidy\n",
    "        tb_col = tb.replace(\"Á\",\"A\")\n",
    "        df[tb][\"level\"] = pd.Series([tb_col] * df[tb].shape[0])\n",
    "            \n",
    "        # creates new year column\n",
    "        df[tb][\"year\"] = pd.Series([year] * df[tb].shape[0])\n",
    "\n",
    "        # melts dataframe to make it tidy\n",
    "        priorities = df[tb].columns[9:14]\n",
    "        df[tb] = pd.melt(df[tb], id_vars = [x for x in df[tb].columns if x not in priorities], value_vars = priorities, var_name = \"priority\", value_name = \"total\")\n",
    "\n",
    "        # changes column names\n",
    "        df[tb].columns = [\"rbd\", \"dv_rbd\", \"school_name\", \"dependency\", \"area\", \"region_code\", \"province_code\", \"commune_code\", \"commune_name\", \"level\", \"year\", \"priority\", \"total\"]\n",
    "        \n",
    "        # drops unnecesary columns: \"dv_rbd\", \"school_name\", \"region_code\", \"province_code\" and \"commune_name\"\n",
    "        df[tb] = df[tb][[\"rbd\", \"dependency\", \"area\", \"commune_code\", \"level\", \"year\", \"priority\", \"total\"]]\n",
    "        \n",
    "        # changes column types\n",
    "        num_cols = [\"rbd\", \"commune_code\", \"total\"]\n",
    "        df[tb][num_cols] = df[tb][num_cols].apply(pd.to_numeric, downcast=\"integer\")\n",
    "        \n",
    "        # changes string columns to uppercase\n",
    "        def uppercase(row, col):\n",
    "            return row[col].upper()\n",
    "        \n",
    "        str_cols = [x for x in df[tb].columns if x not in num_cols]\n",
    "        \n",
    "        for column in str_cols:\n",
    "            df[tb][column] = df[tb].apply(uppercase, col = column, axis = 1) \n",
    "         \n",
    "    # concatenates both dataframes\n",
    "    df = pd.concat([df[sheets[0]], df[sheets[1]]], ignore_index=True)\n",
    "    years_df.append(df)\n",
    "\n",
    "# function to get year information\n",
    "def get_year(string):\n",
    "    reg = re.search(\"\\d\", string)\n",
    "    first = reg.start()\n",
    "    y = string[first : first + 4]\n",
    "    return y \n",
    "\n",
    "# processes each link\n",
    "for file in os.listdir():\n",
    "    new_filename = file[file.rfind(\"/\") + 1:]\n",
    "    print(\"{:.2f} s | Cleaning {}\".format(time.perf_counter()-before, new_filename))\n",
    "    clean(new_filename, get_year(new_filename))\n",
    "\n",
    "# concatenates each year's dataframe\n",
    "df = pd.concat(years_df, ignore_index=True)\n",
    "print(\"{:.2f} s | Concatenated each year's dataframe.\".format(time.perf_counter()-before))\n",
    "\n",
    "# classifies dependencies: administration\n",
    "def classify_dependencies(row, col):\n",
    "    dep_dict = {\"SUB\": 3, \"CORP\": 1, \"MUNI\": 2, \"DELE\": 5}\n",
    "    return next((dep_dict[k] for k in dep_dict.keys() if k in row[col]), np.nan)\n",
    "    \n",
    "df[\"dependency\"] = df.apply(classify_dependencies, col = \"dependency\", axis = 1)\n",
    "df = df.rename(columns = {\"dependency\":\"administration\"})\n",
    "print(\"{:.2f} s | Classified administration column.\".format(time.perf_counter()-before))\n",
    "\n",
    "# Changes IDs and column names (area: zone)\n",
    "def zone(row, col):\n",
    "    zone_dict = {\"URBANO\": 1, \"RURAL\": 2}\n",
    "    return next((zone_dict[k] for k in zone_dict.keys() if k in row[col]), np.nan)\n",
    "    \n",
    "df[\"area\"] = df.apply(zone, col = \"area\", axis = 1)\n",
    "df = df.rename(columns = {\"area\":\"zone_id\"})\n",
    "print(\"{:.2f} s | Changed zone IDs.\".format(time.perf_counter()-before))\n",
    "\n",
    "# classifies priorities\n",
    "def classify_priorities(row, col):\n",
    "    pri_dict = {\"SIN INFORMACION\": 0, \"SIN INFORMACIÓN\": 0, \"PRIMERA PRIORIDAD\": 1, \"1ª PRIORIDAD\": 1, \"SEGUNDA PRIORIDAD\": 2, \"2ª PRIORIDAD\": 2, \"TERCERA PRIORIDAD\": 3, \"3ª PRIORIDAD\": 3, \"NO VULNERABLES\": 4, \"NO APLICA\": 4}\n",
    "    return pri_dict[row[col]]\n",
    "    \n",
    "df[\"priority\"] = df.apply(classify_priorities, col = \"priority\", axis = 1)\n",
    "df[\"priority\"] = pd.to_numeric(df[\"priority\"], downcast = \"integer\")\n",
    "print(\"{:.2f} s | Classified priority column.\".format(time.perf_counter()-before))\n",
    "\n",
    "# Changes IDs and column names (level: education)\n",
    "def education(row, col):\n",
    "    ed_dict = {\"BASICA\": 1, \"MEDIA\": 2}\n",
    "    return next((ed_dict[k] for k in ed_dict.keys() if k in row[col]), np.nan)\n",
    "    \n",
    "df[\"level\"] = df.apply(education, col = \"level\", axis = 1)\n",
    "df = df.rename(columns = {\"level\":\"education\"})\n",
    "print(\"{:.2f} s | Changed education IDs.\".format(time.perf_counter()-before))\n",
    "\n",
    "# writes datachile official IDs for each commune and drops columns: dv_rbd and school_name\n",
    "df_ids = pd.read_csv(\"https://raw.githubusercontent.com/datachile/datachile-etl/master/official_ids/2017_06_27_comunas_datachile_fixed.csv\")\n",
    "df = pd.merge(df, df_ids, left_on = \"commune_code\", right_on = \"comuna_customs_id\")\n",
    "df = df[[\"rbd\", \"administration\", \"zone_id\", \"comuna_datachile_id\", \"education\", \"year\", \"priority\", \"total\"]]\n",
    "df = df.rename(columns = {\"administration\": \"administration_id\", \"comuna_datachile_id\": \"comuna_id\", \"education\": \"education_id\", \"priority\": \"priority_id\"})\n",
    "\n",
    "# comes back to original path, creates data_final folder and exports as csv\n",
    "os.chdir(absolute_path)\n",
    "if os.path.isdir(\"data_final\") == False:\n",
    "    os.mkdir(\"data_final\")    \n",
    "os.chdir(\"data_final\")\n",
    "df.to_csv(\"ive_junaeb.csv\", index = False)\n",
    "print(\"{:.2f} s | Exported CSV file.\".format(time.perf_counter()-before))\n",
    "\n",
    "# creates CSV with priority IDs\n",
    "pri_tb = {\"id\": list(range(5)), \"name_es\": [\"Sin información\", \"Primera prioridad\", \"Segunda prioridad\", \"Tercera prioridad\", \"No vulnerables\"], \"name_en\": [\"No information\", \"First priority\", \"Second priority\", \"Third priority\", \"Not vulnerable\"]}\n",
    "pri_df = pd.DataFrame(pri_tb)\n",
    "pri_df.to_csv(\"priority.csv\", index = False)\n",
    "print(\"{:.2f} s | Exported priority.csv\".format(time.perf_counter()-before))\n",
    "\n",
    "# comes back to original path\n",
    "os.chdir(absolute_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
